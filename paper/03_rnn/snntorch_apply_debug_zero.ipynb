{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the latest version of snnTorch with a recent fix for the state_dict loading, see [PR315](https://github.com/jeshraghian/snntorch/pull/315)\n",
    "\n",
    "Install via:\n",
    "```\n",
    "pip uninstall snntorch\n",
    "git clone https://github.com/gekkom/snntorch.git\n",
    "git checkout fix-saveload\n",
    "cd snntorch\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import surrogate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVICE SETTINGS\n",
    "use_gpu = False\n",
    "\n",
    "if use_gpu:\n",
    "    gpu_sel = 0\n",
    "    device = torch.device(\"cuda:\"+str(gpu_sel))\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIFY THE RESET MECHANISM TO USE\n",
    "reset_mechanism = \"zero\" # \"zero\" or \"subtract\"\n",
    "reset_delay = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMAL HYPERPARAMETERS\n",
    "if reset_mechanism == \"subtract\":\n",
    "   parameters_path = \"data/parameters_noDelay_noBias_ref_subtract.json\"\n",
    "elif reset_mechanism == \"zero\":\n",
    "   parameters_path = \"data/parameters_noDelay_bias_ref_zero.json\"\n",
    "\n",
    "with open(parameters_path) as f:\n",
    "   parameters = json.load(f)\n",
    "\n",
    "parameters[\"reset\"] = reset_mechanism\n",
    "parameters[\"reset_delay\"] = reset_delay\n",
    "\n",
    "regularization = [parameters[\"reg_l1\"], parameters[\"reg_l2\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINED WEIGHTS\n",
    "if reset_mechanism == \"subtract\":\n",
    "   saved_state_dict_path = \"data/model_noDelay_noBias_ref_subtract.pt\"\n",
    "elif reset_mechanism == \"zero\":\n",
    "   saved_state_dict_path = \"data/model_noDelay_bias_ref_zero.pt\"\n",
    "best_val_layers = torch.load(saved_state_dict_path, map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOSS FUNCTION\n",
    "loss_fn = SF.ce_count_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST DATA\n",
    "test_data_path = \"data/ds_test.pt\"\n",
    "ds_test = torch.load(test_data_path)\n",
    "\n",
    "letter_written = ['Space', 'A', 'E', 'I', 'O', 'U', 'Y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(settings, input_size, num_steps, device):\n",
    "\n",
    "    ### Network structure (input data --> encoding -> hidden -> output)\n",
    "    input_channels = int(input_size)\n",
    "    num_hidden = int(settings[\"nb_hidden\"])\n",
    "    num_outputs = 7\n",
    "\n",
    "    ### Surrogate gradient setting\n",
    "    spike_grad = surrogate.fast_sigmoid(slope=int(settings[\"slope\"]))\n",
    "\n",
    "    ### Put things together\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            ##### Initialize layers #####\n",
    "            self.fc1 = nn.Linear(input_channels, num_hidden)\n",
    "            self.fc1.__setattr__(\"bias\",None)\n",
    "            #self.lif1 = snn.RLeaky(beta=settings[\"beta_r\"], linear_features=num_hidden, spike_grad=spike_grad, reset_mechanism=settings[\"reset\"])\n",
    "            self.lif1 = snn.RSynaptic(alpha=settings[\"alpha_r\"], beta=settings[\"beta_r\"], linear_features=num_hidden, spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "            self.lif1.recurrent.__setattr__(\"bias\",None)\n",
    "            ### Output layer\n",
    "            self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "            self.fc2.__setattr__(\"bias\",None)\n",
    "            #self.lif2 = snn.Leaky(beta=settings[\"beta_out\"], reset_mechanism=settings[\"reset\"])\n",
    "            self.lif2 = snn.Synaptic(alpha=settings[\"alpha_out\"], beta=settings[\"beta_out\"], spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            ##### Initialize hidden states at t=0 #####\n",
    "            #spk1, mem1 = self.lif1.init_rleaky()\n",
    "            spk1, syn1, mem1 = self.lif1.init_rsynaptic()\n",
    "            #mem2 = self.lif2.init_leaky()\n",
    "            syn2, mem2 = self.lif2.init_synaptic()\n",
    "\n",
    "            # Record the spikes from the hidden layer (if needed)\n",
    "            spk1_rec = [] # not necessarily needed for inference\n",
    "            # Record the final layer\n",
    "            spk2_rec = []\n",
    "            #syn2_rec = [] # not necessarily needed for inference\n",
    "            #mem2_rec = [] # not necessarily needed for inference\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                ### Recurrent layer\n",
    "                cur1 = self.fc1(x[step])\n",
    "                #spk1, mem1 = self.lif1(cur1, spk1, mem1)\n",
    "                spk1, syn1, mem1 = self.lif1(cur1, spk1, syn1, mem1)\n",
    "                ### Output layer\n",
    "                cur2 = self.fc2(spk1)\n",
    "                #spk2, mem2 = self.lif2(cur2, mem2)\n",
    "                spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "\n",
    "                spk1_rec.append(spk1) # not necessarily needed for inference\n",
    "                spk2_rec.append(spk2)\n",
    "                #syn2_rec.append(mem2) # not necessarily needed for inference\n",
    "                #mem2_rec.append(mem2) # not necessarily needed for inference\n",
    "\n",
    "            return torch.stack(spk2_rec, dim=0), torch.stack(spk1_rec, dim=0)\n",
    "\n",
    "    return Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_test_loop(dataset, batch_size, net, loss_fn, device, shuffle=True, saved_state_dict=None, label_probabilities=False, regularization=None):\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    net.eval()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "\n",
    "    for data, labels in loader:\n",
    "        if data.shape[0] != batch_size:\n",
    "            continue\n",
    "        data = data.to(device).swapaxes(1, 0)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        spk_out, hid_rec = net(data)\n",
    "\n",
    "        # Validation loss\n",
    "        if regularization != None:\n",
    "            # L1 loss on spikes per neuron from the hidden layer\n",
    "            reg_loss = regularization[0]*torch.mean(torch.sum(hid_rec, 0))\n",
    "            # L2 loss on total number of spikes from the hidden layer\n",
    "            reg_loss = reg_loss + regularization[1]*torch.mean(torch.sum(torch.sum(hid_rec, dim=0), dim=1)**2)\n",
    "            loss_val = loss_fn(spk_out, labels) + reg_loss\n",
    "        else:\n",
    "            loss_val = loss_fn(spk_out, labels)\n",
    "\n",
    "        batch_loss.append(loss_val.detach().cpu().item())\n",
    "\n",
    "        # Accuracy\n",
    "        act_total_out = torch.sum(spk_out, 0)  # sum over time\n",
    "        _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
    "        batch_acc.extend((neuron_max_act_total_out == labels).detach().cpu().numpy()) # batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
    "    \n",
    "    if label_probabilities:\n",
    "        log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "        log_p_y = log_softmax_fn(act_total_out)\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)], torch.exp(log_p_y)\n",
    "    else:\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = best_val_layers.copy()\n",
    "if reset_mechanism == \"zero\":\n",
    "    best_val_layers.pop(\"fc1.bias\")\n",
    "    best_val_layers.pop(\"lif1.recurrent.bias\")\n",
    "    best_val_layers.pop(\"fc2.bias\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.28%\n"
     ]
    }
   ],
   "source": [
    "### INFERENCE ON TEST SET\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "input_size = 12 \n",
    "num_steps = next(iter(ds_test))[0].shape[0]\n",
    "\n",
    "net = model_build(parameters, input_size, num_steps, device)\n",
    "\n",
    "net.load_state_dict(best_val_layers)\n",
    "\n",
    "net._modules['fc1'].bias = nn.Parameter(sd['fc1.bias'])\n",
    "net._modules['lif1'].recurrent.bias = nn.Parameter(sd['lif1.recurrent.bias'])\n",
    "net._modules['fc2'].bias = nn.Parameter(sd['fc2.bias'])\n",
    "\n",
    "test_results = val_test_loop(ds_test, batch_size, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, regularization=regularization)\n",
    "\n",
    "print(\"Test accuracy: {}%\".format(np.round(test_results[1]*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-sample inference 1/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[0.000e+00 0.000e+00 0.000e+00 9.000e-02 0.000e+00 0.000e+00 9.991e+01]]\n",
      "\n",
      "Single-sample inference 2/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[ 0.    0.25  0.    0.67  0.    0.   99.09]]\n",
      "\n",
      "Single-sample inference 3/10 from test set:\n",
      "Sample: O \tPrediction: O\n",
      "Label probabilities (%): [[  0.   0.   0.   0. 100.   0.   0.]]\n",
      "\n",
      "Single-sample inference 4/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 3.000e-02 9.997e+01]]\n",
      "\n",
      "Single-sample inference 5/10 from test set:\n",
      "Sample: I \tPrediction: I\n",
      "Label probabilities (%): [[ 0.    0.   11.92 88.08  0.    0.    0.  ]]\n",
      "\n",
      "Single-sample inference 6/10 from test set:\n",
      "Sample: A \tPrediction: A\n",
      "Label probabilities (%): [[  0. 100.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 7/10 from test set:\n",
      "Sample: I \tPrediction: I\n",
      "Label probabilities (%): [[0.000e+00 0.000e+00 3.000e-02 9.997e+01 0.000e+00 0.000e+00 0.000e+00]]\n",
      "\n",
      "Single-sample inference 8/10 from test set:\n",
      "Sample: Space \tPrediction: Space\n",
      "Label probabilities (%): [[9.525e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 4.740e+00 1.000e-02]]\n",
      "\n",
      "Single-sample inference 9/10 from test set:\n",
      "Sample: O \tPrediction: I\n",
      "Label probabilities (%): [[0.000e+00 0.000e+00 4.740e+00 9.517e+01 0.000e+00 0.000e+00 9.000e-02]]\n",
      "\n",
      "Single-sample inference 10/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[0.000e+00 0.000e+00 0.000e+00 1.000e-02 0.000e+00 0.000e+00 9.999e+01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### INFERENCE ON INDIVIDUAL TEST SAMPLES\n",
    "\n",
    "Ns = 10\n",
    "\n",
    "for ii in range(Ns):\n",
    "\n",
    "    single_sample = next(iter(DataLoader(ds_test, batch_size=1, shuffle=True)))\n",
    "    _, lbl_probs = val_test_loop(TensorDataset(single_sample[0],single_sample[1]), 1, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, label_probabilities=True, regularization=regularization)\n",
    "    print(\"Single-sample inference {}/{} from test set:\".format(ii+1,Ns))\n",
    "    print(\"Sample: {} \\tPrediction: {}\".format(letter_written[single_sample[1]],letter_written[torch.max(lbl_probs.cpu(),1)[1]]))\n",
    "    print(\"Label probabilities (%): {}\\n\".format(np.round(np.array(lbl_probs.detach().cpu().numpy())*100,2)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
