{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import surrogate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "store_weights = False\n",
    "\n",
    "training_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVICE SETTINGS\n",
    "use_gpu = False\n",
    "\n",
    "if use_gpu:\n",
    "    gpu_sel = 0\n",
    "    device = torch.device(\"cuda:\"+str(gpu_sel))\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIFY THE RESET MECHANISM TO USE AND WHETHER TO DELAY IT OR NOT\n",
    "reset_mechanism = \"subtract\" # \"zero\" or \"subtract\"\n",
    "reset_delay = False # True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET BIAS USAGE DEPENDING ON THE TARGET PLATFORM\n",
    "use_bias = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMAL HYPERPARAMETERS\n",
    "parameters_filename = \"parameters_ref_{}.json\".format(reset_mechanism)\n",
    "parameters_path = \"data/{}\".format(parameters_filename)\n",
    "\n",
    "with open(parameters_path) as f:\n",
    "   parameters = json.load(f)\n",
    "\n",
    "parameters[\"reset\"] = reset_mechanism\n",
    "parameters[\"reset_delay\"] = reset_delay\n",
    "\n",
    "parameters[\"use_bias\"] = use_bias\n",
    "\n",
    "regularization = [parameters[\"reg_l1\"], parameters[\"reg_l2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA\n",
    "ds_train = torch.load(\"data/ds_train.pt\")\n",
    "ds_val = torch.load(\"data/ds_train.pt\")\n",
    "ds_test = torch.load(\"data/ds_test.pt\")\n",
    "\n",
    "letter_written = ['Space', 'A', 'E', 'I', 'O', 'U', 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(settings, input_size, num_steps, device):\n",
    "\n",
    "    input_channels = int(input_size)\n",
    "    num_hidden = int(settings[\"nb_hidden\"])\n",
    "    num_outputs = 7\n",
    "\n",
    "    ### Surrogate gradient setting\n",
    "    spike_grad = surrogate.fast_sigmoid(slope=int(settings[\"slope\"]))\n",
    "\n",
    "    ### Put things together\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            ##### Initialize layers #####\n",
    "            self.fc1 = nn.Linear(input_channels, num_hidden)\n",
    "            if not settings[\"use_bias\"]:\n",
    "                self.fc1.__setattr__(\"bias\",None)\n",
    "            self.lif1 = snn.RSynaptic(alpha=settings[\"alpha_r\"], beta=settings[\"beta_r\"], linear_features=num_hidden, spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "            if not settings[\"use_bias\"]:\n",
    "                self.lif1.recurrent.__setattr__(\"bias\",None)\n",
    "            ### Output layer\n",
    "            self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "            if not settings[\"use_bias\"]:\n",
    "                self.fc2.__setattr__(\"bias\",None)\n",
    "            self.lif2 = snn.Synaptic(alpha=settings[\"alpha_out\"], beta=settings[\"beta_out\"], spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            ##### Initialize hidden states at t=0 #####\n",
    "            #spk1, mem1 = self.lif1.init_rleaky()\n",
    "            spk1, syn1, mem1 = self.lif1.init_rsynaptic()\n",
    "            #mem2 = self.lif2.init_leaky()\n",
    "            syn2, mem2 = self.lif2.init_synaptic()\n",
    "\n",
    "            # Record the spikes from the hidden layer (if needed)\n",
    "            spk1_rec = [] # not necessarily needed for inference\n",
    "            # Record the final layer\n",
    "            spk2_rec = []\n",
    "            #syn2_rec = [] # not necessarily needed for inference\n",
    "            #mem2_rec = [] # not necessarily needed for inference\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                ### Recurrent layer\n",
    "                cur1 = self.fc1(x[step])\n",
    "                #spk1, mem1 = self.lif1(cur1, spk1, mem1)\n",
    "                spk1, syn1, mem1 = self.lif1(cur1, spk1, syn1, mem1)\n",
    "                ### Output layer\n",
    "                cur2 = self.fc2(spk1)\n",
    "                #spk2, mem2 = self.lif2(cur2, mem2)\n",
    "                spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "\n",
    "                spk1_rec.append(spk1) # not necessarily needed for inference\n",
    "                spk2_rec.append(spk2)\n",
    "                #syn2_rec.append(mem2) # not necessarily needed for inference\n",
    "                #mem2_rec.append(mem2) # not necessarily needed for inference\n",
    "\n",
    "            return torch.stack(spk2_rec, dim=0), torch.stack(spk1_rec, dim=0)\n",
    "\n",
    "    return Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataset, batch_size, net, optimizer, loss_fn, device, regularization=None):\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "      \n",
    "      data = data.to(device).swapaxes(1, 0)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      net.train()\n",
    "      #spk_rec, _, _, hid_rec = net(data)\n",
    "      spk_rec, hid_rec = net(data)\n",
    "\n",
    "      # Training loss\n",
    "      if regularization != None:\n",
    "        # L1 loss on spikes per neuron from the hidden layer\n",
    "        reg_loss = regularization[0]*torch.mean(torch.sum(hid_rec, 0))\n",
    "        # L2 loss on total number of spikes from the hidden layer\n",
    "        reg_loss = reg_loss + regularization[1]*torch.mean(torch.sum(torch.sum(hid_rec, dim=0), dim=1)**2)\n",
    "        loss_val = loss_fn(spk_rec, labels) + reg_loss\n",
    "      else:\n",
    "        loss_val = loss_fn(spk_rec, labels)\n",
    "\n",
    "      batch_loss.append(loss_val.detach().cpu().item())\n",
    "\n",
    "      # Training accuracy\n",
    "      act_total_out = torch.sum(spk_rec, 0)  # sum over time\n",
    "      _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
    "      batch_acc.extend((neuron_max_act_total_out == labels).detach().cpu().numpy()) # the \"old\" one with mean per batch: batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
    "\n",
    "      # Gradient calculation + weight update\n",
    "      optimizer.zero_grad()\n",
    "      loss_val.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    epoch_loss = np.mean(batch_loss)\n",
    "    epoch_acc = np.mean(batch_acc)\n",
    "    \n",
    "    return [epoch_loss, epoch_acc]\n",
    "\n",
    "\n",
    "def val_test_loop(dataset, batch_size, net, loss_fn, device, shuffle=True, saved_state_dict=None, label_probabilities=False, regularization=None):\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    if saved_state_dict != None:\n",
    "        net.load_state_dict(saved_state_dict)\n",
    "    net.eval()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "\n",
    "    for data, labels in loader:\n",
    "        data = data.to(device).swapaxes(1, 0)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        spk_out, hid_rec = net(data)\n",
    "\n",
    "        # Validation loss\n",
    "        if regularization != None:\n",
    "            # L1 loss on spikes per neuron from the hidden layer\n",
    "            reg_loss = regularization[0]*torch.mean(torch.sum(hid_rec, 0))\n",
    "            # L2 loss on total number of spikes from the hidden layer\n",
    "            reg_loss = reg_loss + regularization[1]*torch.mean(torch.sum(torch.sum(hid_rec, dim=0), dim=1)**2)\n",
    "            loss_val = loss_fn(spk_out, labels) + reg_loss\n",
    "        else:\n",
    "            loss_val = loss_fn(spk_out, labels)\n",
    "\n",
    "        batch_loss.append(loss_val.detach().cpu().item())\n",
    "\n",
    "        # Accuracy\n",
    "        act_total_out = torch.sum(spk_out, 0)  # sum over time\n",
    "        _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
    "        batch_acc.extend((neuron_max_act_total_out == labels).detach().cpu().numpy()) # batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
    "    \n",
    "    if label_probabilities:\n",
    "        log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "        log_p_y = log_softmax_fn(act_total_out)\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)], torch.exp(log_p_y)\n",
    "    else:\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE FOR TRAINING\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "input_size = 12 \n",
    "num_steps = next(iter(ds_test))[0].shape[0]\n",
    "\n",
    "net = model_build(parameters, input_size, num_steps, device)\n",
    "\n",
    "loss_fn = SF.ce_count_loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=parameters[\"lr\"], betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING (with validation and test)\n",
    "\n",
    "print(\"Training started on: {}-{}-{} {}:{}:{}\\n\".format(\n",
    "    training_datetime[:4],\n",
    "    training_datetime[4:6],\n",
    "    training_datetime[6:8],\n",
    "    training_datetime[-6:-4],\n",
    "    training_datetime[-4:-2],\n",
    "    training_datetime[-2:])\n",
    "    )\n",
    "\n",
    "training_results = []\n",
    "validation_results = []\n",
    "\n",
    "for ee in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = training_loop(ds_train, batch_size, net, optimizer, loss_fn, device, regularization=regularization)\n",
    "    val_loss, val_acc = val_test_loop(ds_val, batch_size, net, loss_fn, device, regularization=regularization)\n",
    "\n",
    "    training_results.append([train_loss, train_acc])\n",
    "    validation_results.append([val_loss, val_acc])\n",
    "\n",
    "    if (ee == 0) | ((ee+1)%10 == 0):\n",
    "        print(\"\\tepoch {}/{} done \\t --> \\ttraining accuracy (loss): {}% ({}), \\tvalidation accuracy (loss): {}% ({})\".format(ee+1,num_epochs,np.round(training_results[-1][1]*100,4), training_results[-1][0], np.round(validation_results[-1][1]*100,4), validation_results[-1][0]))\n",
    "        \n",
    "    if val_acc >= np.max(np.array(validation_results)[:,1]):\n",
    "        best_val_layers = copy.deepcopy(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hist = np.array(training_results)\n",
    "validation_hist = np.array(validation_results)\n",
    "\n",
    "# best training and validation at best training\n",
    "acc_best_train = np.max(training_hist[:,1])\n",
    "epoch_best_train = np.argmax(training_hist[:,1])\n",
    "acc_val_at_best_train = validation_hist[epoch_best_train][1]\n",
    "\n",
    "# best validation and training at best validation\n",
    "acc_best_val = np.max(validation_hist[:,1])\n",
    "epoch_best_val = np.argmax(validation_hist[:,1])\n",
    "acc_train_at_best_val = training_hist[epoch_best_val][1]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Overall results:\")\n",
    "print(\"\\tBest training accuracy: {}% ({}% corresponding validation accuracy) at epoch {}/{}\".format(\n",
    "    np.round(acc_best_train*100,4), np.round(acc_val_at_best_train*100,4), epoch_best_train+1, num_epochs))\n",
    "print(\"\\tBest validation accuracy: {}% ({}% corresponding training accuracy) at epoch {}/{}\".format(\n",
    "    np.round(acc_best_val*100,4), np.round(acc_train_at_best_val*100,4), epoch_best_val+1, num_epochs))\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Test\n",
    "test_results = val_test_loop(ds_test, batch_size, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, regularization=regularization)\n",
    "print(\"Test accuracy: {}%\\n\".format(np.round(test_results[1]*100,2)))\n",
    "\n",
    "# Ns single-sample inferences to check label probabilities\n",
    "Ns = 10\n",
    "for ii in range(Ns):\n",
    "    single_sample = next(iter(DataLoader(ds_test, batch_size=1, shuffle=True)))\n",
    "    _, lbl_probs = val_test_loop(TensorDataset(single_sample[0],single_sample[1]), 1, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, label_probabilities=True, regularization=regularization)\n",
    "    print(\"Single-sample inference {}/{} from test set:\".format(ii+1,Ns))\n",
    "    print(\"Sample: {} \\tPrediction: {}\".format(letter_written[single_sample[1]],letter_written[torch.max(lbl_probs.cpu(),1)[1]]))\n",
    "    print(\"Label probabilities (%): {}\\n\".format(np.round(np.array(lbl_probs.detach().cpu().numpy())*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the trained weights\n",
    "if store_weights:\n",
    "    torch.save(best_val_layers, \"data/retrained_snntorch_{}.pt\".format(training_datetime))\n",
    "    print(\"*** weights stored ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_NIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
