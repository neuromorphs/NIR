{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import surrogate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "store_weights = False\n",
    "\n",
    "training_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVICE SETTINGS\n",
    "use_gpu = False\n",
    "\n",
    "if use_gpu:\n",
    "    gpu_sel = 0\n",
    "    device = torch.device(\"cuda:\"+str(gpu_sel))\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIFY THE RESET MECHANISM TO USE AND WHETHER TO DELAY IT OR NOT\n",
    "reset_mechanism = \"zero\" # \"zero\" or \"subtract\"\n",
    "reset_delay = False # True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMAL HYPERPARAMETERS\n",
    "parameters_path = \"data/parameters_ref_{}.json\".format(reset_mechanism)\n",
    "\n",
    "with open(parameters_path) as f:\n",
    "   parameters = json.load(f)\n",
    "\n",
    "parameters[\"reset\"] = reset_mechanism\n",
    "parameters[\"reset_delay\"] = reset_delay\n",
    "\n",
    "regularization = [parameters[\"reg_l1\"], parameters[\"reg_l2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA\n",
    "ds_train = torch.load(\"data/ds_train.pt\")\n",
    "ds_val = torch.load(\"data/ds_train.pt\")\n",
    "ds_test = torch.load(\"data/ds_test.pt\")\n",
    "\n",
    "letter_written = ['Space', 'A', 'E', 'I', 'O', 'U', 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(settings, input_size, num_steps, device):\n",
    "\n",
    "    ### Network structure (input data --> encoding -> hidden -> output)\n",
    "    input_channels = int(input_size)\n",
    "    num_hidden = int(settings[\"nb_hidden\"])\n",
    "    num_outputs = 7\n",
    "\n",
    "    ### Surrogate gradient setting\n",
    "    spike_grad = surrogate.fast_sigmoid(slope=int(settings[\"slope\"]))\n",
    "\n",
    "    ### Put things together\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            ##### Initialize layers #####\n",
    "            self.fc1 = nn.Linear(input_channels, num_hidden)\n",
    "            #self.lif1 = snn.RLeaky(beta=settings[\"beta_r\"], linear_features=num_hidden, spike_grad=spike_grad, reset_mechanism=settings[\"reset\"])\n",
    "            self.lif1 = snn.RSynaptic(alpha=settings[\"alpha_r\"], beta=settings[\"beta_r\"], linear_features=num_hidden, spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "            self.lif1.recurrent.__setattr__(\"bias\",None)\n",
    "            ### Output layer\n",
    "            self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "            #self.lif2 = snn.Leaky(beta=settings[\"beta_out\"], reset_mechanism=settings[\"reset\"])\n",
    "            self.lif2 = snn.Synaptic(alpha=settings[\"alpha_out\"], beta=settings[\"beta_out\"], spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            ##### Initialize hidden states at t=0 #####\n",
    "            #spk1, mem1 = self.lif1.init_rleaky()\n",
    "            spk1, syn1, mem1 = self.lif1.init_rsynaptic()\n",
    "            #mem2 = self.lif2.init_leaky()\n",
    "            syn2, mem2 = self.lif2.init_synaptic()\n",
    "\n",
    "            # Record the spikes from the hidden layer (if needed)\n",
    "            spk1_rec = [] # not necessarily needed for inference\n",
    "            # Record the final layer\n",
    "            spk2_rec = []\n",
    "            #syn2_rec = [] # not necessarily needed for inference\n",
    "            #mem2_rec = [] # not necessarily needed for inference\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                ### Recurrent layer\n",
    "                cur1 = self.fc1(x[step])\n",
    "                #spk1, mem1 = self.lif1(cur1, spk1, mem1)\n",
    "                spk1, syn1, mem1 = self.lif1(cur1, spk1, syn1, mem1)\n",
    "                ### Output layer\n",
    "                cur2 = self.fc2(spk1)\n",
    "                #spk2, mem2 = self.lif2(cur2, mem2)\n",
    "                spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "\n",
    "                spk1_rec.append(spk1) # not necessarily needed for inference\n",
    "                spk2_rec.append(spk2)\n",
    "                #syn2_rec.append(mem2) # not necessarily needed for inference\n",
    "                #mem2_rec.append(mem2) # not necessarily needed for inference\n",
    "\n",
    "            return torch.stack(spk2_rec, dim=0), torch.stack(spk1_rec, dim=0)\n",
    "\n",
    "    return Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataset, batch_size, net, optimizer, loss_fn, device, regularization=None):\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "      \n",
    "      data = data.to(device).swapaxes(1, 0)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      net.train()\n",
    "      #spk_rec, _, _, hid_rec = net(data)\n",
    "      spk_rec, hid_rec = net(data)\n",
    "\n",
    "      # Training loss\n",
    "      if regularization != None:\n",
    "        # L1 loss on spikes per neuron from the hidden layer\n",
    "        reg_loss = regularization[0]*torch.mean(torch.sum(hid_rec, 0))\n",
    "        # L2 loss on total number of spikes from the hidden layer\n",
    "        reg_loss = reg_loss + regularization[1]*torch.mean(torch.sum(torch.sum(hid_rec, dim=0), dim=1)**2)\n",
    "        loss_val = loss_fn(spk_rec, labels) + reg_loss\n",
    "      else:\n",
    "        loss_val = loss_fn(spk_rec, labels)\n",
    "\n",
    "      batch_loss.append(loss_val.detach().cpu().item())\n",
    "\n",
    "      # Training accuracy\n",
    "      act_total_out = torch.sum(spk_rec, 0)  # sum over time\n",
    "      _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
    "      batch_acc.extend((neuron_max_act_total_out == labels).detach().cpu().numpy()) # the \"old\" one with mean per batch: batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
    "\n",
    "      # Gradient calculation + weight update\n",
    "      optimizer.zero_grad()\n",
    "      loss_val.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    epoch_loss = np.mean(batch_loss)\n",
    "    epoch_acc = np.mean(batch_acc)\n",
    "    \n",
    "    return [epoch_loss, epoch_acc]\n",
    "\n",
    "\n",
    "def val_test_loop(dataset, batch_size, net, loss_fn, device, shuffle=True, saved_state_dict=None, label_probabilities=False, regularization=None):\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    if saved_state_dict != None:\n",
    "        net.load_state_dict(saved_state_dict)\n",
    "    net.eval()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "\n",
    "    for data, labels in loader:\n",
    "        data = data.to(device).swapaxes(1, 0)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        spk_out, hid_rec = net(data)\n",
    "\n",
    "        # Validation loss\n",
    "        if regularization != None:\n",
    "            # L1 loss on spikes per neuron from the hidden layer\n",
    "            reg_loss = regularization[0]*torch.mean(torch.sum(hid_rec, 0))\n",
    "            # L2 loss on total number of spikes from the hidden layer\n",
    "            reg_loss = reg_loss + regularization[1]*torch.mean(torch.sum(torch.sum(hid_rec, dim=0), dim=1)**2)\n",
    "            loss_val = loss_fn(spk_out, labels) + reg_loss\n",
    "        else:\n",
    "            loss_val = loss_fn(spk_out, labels)\n",
    "\n",
    "        batch_loss.append(loss_val.detach().cpu().item())\n",
    "\n",
    "        # Accuracy\n",
    "        act_total_out = torch.sum(spk_out, 0)  # sum over time\n",
    "        _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
    "        batch_acc.extend((neuron_max_act_total_out == labels).detach().cpu().numpy()) # batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
    "    \n",
    "    if label_probabilities:\n",
    "        log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "        log_p_y = log_softmax_fn(act_total_out)\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)], torch.exp(log_p_y)\n",
    "    else:\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE FOR TRAINING\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "input_size = 12 \n",
    "num_steps = next(iter(ds_test))[0].shape[0]\n",
    "\n",
    "net = model_build(parameters, input_size, num_steps, device)\n",
    "\n",
    "loss_fn = SF.ce_count_loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=parameters[\"lr\"], betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started on: 2023-10-24 11:08:06\n",
      "\n",
      "\tepoch 1/500 done \t --> \ttraining accuracy (loss): 12.2449% (18.16168211400509), \tvalidation accuracy (loss): 14.2857% (3.2492939680814743)\n",
      "\tepoch 10/500 done \t --> \ttraining accuracy (loss): 55.7143% (1.4362128712236881), \tvalidation accuracy (loss): 56.8367% (1.7030190825462341)\n",
      "\tepoch 20/500 done \t --> \ttraining accuracy (loss): 77.1429% (1.1853706054389477), \tvalidation accuracy (loss): 74.0816% (1.313743781298399)\n",
      "\tepoch 30/500 done \t --> \ttraining accuracy (loss): 80.7143% (0.7106738314032555), \tvalidation accuracy (loss): 81.7347% (0.7131614610552788)\n",
      "\tepoch 40/500 done \t --> \ttraining accuracy (loss): 85.102% (0.6015189085155725), \tvalidation accuracy (loss): 84.898% (0.6643356736749411)\n",
      "\tepoch 50/500 done \t --> \ttraining accuracy (loss): 86.8367% (0.48365985974669456), \tvalidation accuracy (loss): 87.551% (0.4372143102809787)\n",
      "\tepoch 60/500 done \t --> \ttraining accuracy (loss): 88.1633% (0.4218375002965331), \tvalidation accuracy (loss): 89.6939% (0.41933281999081373)\n",
      "\tepoch 70/500 done \t --> \ttraining accuracy (loss): 89.5918% (0.3825262663885951), \tvalidation accuracy (loss): 90.9184% (0.35189109575003386)\n",
      "\tepoch 80/500 done \t --> \ttraining accuracy (loss): 89.3878% (0.5056350808590651), \tvalidation accuracy (loss): 87.9592% (0.42929896991699934)\n",
      "\tepoch 90/500 done \t --> \ttraining accuracy (loss): 90.7143% (0.34304975159466267), \tvalidation accuracy (loss): 90.8163% (0.3590352088212967)\n",
      "\tepoch 100/500 done \t --> \ttraining accuracy (loss): 89.4898% (0.4129009544849396), \tvalidation accuracy (loss): 88.9796% (0.4059465629979968)\n",
      "\tepoch 110/500 done \t --> \ttraining accuracy (loss): 91.9388% (0.33683993853628635), \tvalidation accuracy (loss): 91.2245% (0.3332856595516205)\n",
      "\tepoch 120/500 done \t --> \ttraining accuracy (loss): 93.1633% (0.2854106239974499), \tvalidation accuracy (loss): 91.9388% (0.3269132534042001)\n",
      "\tepoch 130/500 done \t --> \ttraining accuracy (loss): 93.5714% (0.23396545136347413), \tvalidation accuracy (loss): 92.7551% (0.26929658744484186)\n",
      "\tepoch 140/500 done \t --> \ttraining accuracy (loss): 93.6735% (0.2804742702282965), \tvalidation accuracy (loss): 92.8571% (0.25368758058175445)\n",
      "\tepoch 150/500 done \t --> \ttraining accuracy (loss): 94.6939% (0.23736538318917155), \tvalidation accuracy (loss): 93.3673% (0.29252045415341854)\n",
      "\tepoch 160/500 done \t --> \ttraining accuracy (loss): 94.0816% (0.24243150930851698), \tvalidation accuracy (loss): 94.1837% (0.2550487513653934)\n",
      "\tepoch 170/500 done \t --> \ttraining accuracy (loss): 92.9592% (0.293390859849751), \tvalidation accuracy (loss): 94.898% (0.19862248748540878)\n",
      "\tepoch 180/500 done \t --> \ttraining accuracy (loss): 93.7755% (0.25074308831244707), \tvalidation accuracy (loss): 93.7755% (0.2939768205396831)\n",
      "\tepoch 190/500 done \t --> \ttraining accuracy (loss): 93.5714% (0.2682369560934603), \tvalidation accuracy (loss): 92.7551% (0.36196807585656643)\n",
      "\tepoch 200/500 done \t --> \ttraining accuracy (loss): 94.2857% (0.22802603896707296), \tvalidation accuracy (loss): 94.5918% (0.2270404458977282)\n",
      "\tepoch 210/500 done \t --> \ttraining accuracy (loss): 95.2041% (0.2587066716514528), \tvalidation accuracy (loss): 95.6122% (0.1973765988368541)\n",
      "\tepoch 220/500 done \t --> \ttraining accuracy (loss): 95.9184% (0.21574945305474102), \tvalidation accuracy (loss): 94.6939% (0.2718346454203129)\n",
      "\tepoch 230/500 done \t --> \ttraining accuracy (loss): 94.1837% (0.2992962650023401), \tvalidation accuracy (loss): 96.5306% (0.166609758278355)\n",
      "\tepoch 240/500 done \t --> \ttraining accuracy (loss): 96.0204% (0.15606782119721174), \tvalidation accuracy (loss): 96.4286% (0.14154494600370526)\n",
      "\tepoch 250/500 done \t --> \ttraining accuracy (loss): 94.0816% (0.3013189877383411), \tvalidation accuracy (loss): 94.4898% (0.21260127215646207)\n",
      "\tepoch 260/500 done \t --> \ttraining accuracy (loss): 90.7143% (0.5367089537903666), \tvalidation accuracy (loss): 93.4694% (0.3334489311091602)\n",
      "\tepoch 270/500 done \t --> \ttraining accuracy (loss): 95.6122% (0.21189628797583282), \tvalidation accuracy (loss): 93.7755% (0.2812230079434812)\n",
      "\tepoch 280/500 done \t --> \ttraining accuracy (loss): 95.3061% (0.21414874074980617), \tvalidation accuracy (loss): 94.5918% (0.22190442914143205)\n",
      "\tepoch 290/500 done \t --> \ttraining accuracy (loss): 95.9184% (0.22454248531721532), \tvalidation accuracy (loss): 95.7143% (0.23595023667439818)\n",
      "\tepoch 300/500 done \t --> \ttraining accuracy (loss): 92.6531% (0.39141762792132795), \tvalidation accuracy (loss): 92.8571% (0.38259067106992006)\n",
      "\tepoch 310/500 done \t --> \ttraining accuracy (loss): 96.2245% (0.20547513524070382), \tvalidation accuracy (loss): 95.8163% (0.18433412979356945)\n",
      "\tepoch 320/500 done \t --> \ttraining accuracy (loss): 96.6327% (0.17141235689632595), \tvalidation accuracy (loss): 97.2449% (0.17180103063583374)\n",
      "\tepoch 330/500 done \t --> \ttraining accuracy (loss): 95.7143% (0.19429649924859405), \tvalidation accuracy (loss): 98.3673% (0.10046021151356399)\n",
      "\tepoch 340/500 done \t --> \ttraining accuracy (loss): 96.7347% (0.11543825082480907), \tvalidation accuracy (loss): 96.2245% (0.18507117219269276)\n",
      "\tepoch 350/500 done \t --> \ttraining accuracy (loss): 96.3265% (0.15177611098624766), \tvalidation accuracy (loss): 96.3265% (0.152519378811121)\n",
      "\tepoch 360/500 done \t --> \ttraining accuracy (loss): 95.8163% (0.17669607535935938), \tvalidation accuracy (loss): 97.449% (0.10413952497765422)\n",
      "\tepoch 370/500 done \t --> \ttraining accuracy (loss): 96.9388% (0.15094911330379546), \tvalidation accuracy (loss): 95.3061% (0.22810471849516034)\n",
      "\tepoch 380/500 done \t --> \ttraining accuracy (loss): 96.4286% (0.13685280596837401), \tvalidation accuracy (loss): 97.551% (0.09945283900015056)\n",
      "\tepoch 390/500 done \t --> \ttraining accuracy (loss): 96.0204% (0.17381540220230818), \tvalidation accuracy (loss): 95.3061% (0.20512332022190094)\n",
      "\tepoch 400/500 done \t --> \ttraining accuracy (loss): 97.8571% (0.1143298908136785), \tvalidation accuracy (loss): 97.0408% (0.13109464687295258)\n",
      "\tepoch 410/500 done \t --> \ttraining accuracy (loss): 95.7143% (0.18096148362383246), \tvalidation accuracy (loss): 96.5306% (0.1233336883597076)\n",
      "\tepoch 420/500 done \t --> \ttraining accuracy (loss): 98.0612% (0.09588823257945478), \tvalidation accuracy (loss): 97.8571% (0.108257164247334)\n",
      "\tepoch 430/500 done \t --> \ttraining accuracy (loss): 95.102% (0.22713154647499323), \tvalidation accuracy (loss): 92.551% (0.2695628353394568)\n",
      "\tepoch 440/500 done \t --> \ttraining accuracy (loss): 95.8163% (0.17347429622896016), \tvalidation accuracy (loss): 95.8163% (0.18973152176477015)\n",
      "\tepoch 450/500 done \t --> \ttraining accuracy (loss): 95.2041% (0.18852063769008964), \tvalidation accuracy (loss): 96.9388% (0.13986757607199252)\n",
      "\tepoch 460/500 done \t --> \ttraining accuracy (loss): 95.102% (0.2217071137856692), \tvalidation accuracy (loss): 94.7959% (0.23191623389720917)\n",
      "\tepoch 470/500 done \t --> \ttraining accuracy (loss): 97.8571% (0.12386571150273085), \tvalidation accuracy (loss): 96.3265% (0.13122106669470668)\n",
      "\tepoch 480/500 done \t --> \ttraining accuracy (loss): 97.6531% (0.1116610683966428), \tvalidation accuracy (loss): 96.8367% (0.1272633564658463)\n",
      "\tepoch 490/500 done \t --> \ttraining accuracy (loss): 94.3878% (0.28652571560814977), \tvalidation accuracy (loss): 98.3673% (0.08307758485898376)\n",
      "\tepoch 500/500 done \t --> \ttraining accuracy (loss): 94.5918% (0.21846492728218436), \tvalidation accuracy (loss): 94.5918% (0.2508199787698686)\n"
     ]
    }
   ],
   "source": [
    "### TRAINING (with validation and test)\n",
    "\n",
    "print(\"Training started on: {}-{}-{} {}:{}:{}\\n\".format(\n",
    "    training_datetime[:4],\n",
    "    training_datetime[4:6],\n",
    "    training_datetime[6:8],\n",
    "    training_datetime[-6:-4],\n",
    "    training_datetime[-4:-2],\n",
    "    training_datetime[-2:])\n",
    "    )\n",
    "\n",
    "training_results = []\n",
    "validation_results = []\n",
    "\n",
    "for ee in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = training_loop(ds_train, batch_size, net, optimizer, loss_fn, device, regularization=regularization)\n",
    "    val_loss, val_acc = val_test_loop(ds_val, batch_size, net, loss_fn, device, regularization=regularization)\n",
    "\n",
    "    training_results.append([train_loss, train_acc])\n",
    "    validation_results.append([val_loss, val_acc])\n",
    "\n",
    "    if (ee == 0) | ((ee+1)%10 == 0):\n",
    "        print(\"\\tepoch {}/{} done \\t --> \\ttraining accuracy (loss): {}% ({}), \\tvalidation accuracy (loss): {}% ({})\".format(ee+1,num_epochs,np.round(training_results[-1][1]*100,4), training_results[-1][0], np.round(validation_results[-1][1]*100,4), validation_results[-1][0]))\n",
    "        \n",
    "    if val_acc >= np.max(np.array(validation_results)[:,1]):\n",
    "        best_val_layers = copy.deepcopy(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Overall results:\n",
      "\tBest training accuracy: 98.1633% (96.1224% corresponding validation accuracy) at epoch 471/500\n",
      "\tBest validation accuracy: 98.3673% (95.7143% corresponding training accuracy) at epoch 330/500\n",
      "\n",
      "\n",
      "Test accuracy: 95.0%\n",
      "\n",
      "Single-sample inference 1/10 from test set:\n",
      "Sample: E \tPrediction: E\n",
      "Label probabilities (%): [[ 0.    0.25 99.75  0.    0.    0.    0.  ]]\n",
      "\n",
      "Single-sample inference 2/10 from test set:\n",
      "Sample: I \tPrediction: O\n",
      "Label probabilities (%): [[0.000e+00 0.000e+00 1.000e-02 0.000e+00 9.999e+01 0.000e+00 0.000e+00]]\n",
      "\n",
      "Single-sample inference 3/10 from test set:\n",
      "Sample: U \tPrediction: U\n",
      "Label probabilities (%): [[  0.   0.   0.   0.   0. 100.   0.]]\n",
      "\n",
      "Single-sample inference 4/10 from test set:\n",
      "Sample: I \tPrediction: I\n",
      "Label probabilities (%): [[  0.   0.   0. 100.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 5/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[  0.   0.   0.   0.   0.   0. 100.]]\n",
      "\n",
      "Single-sample inference 6/10 from test set:\n",
      "Sample: O \tPrediction: E\n",
      "Label probabilities (%): [[ 0.    0.   73.1   0.   26.89  0.    0.  ]]\n",
      "\n",
      "Single-sample inference 7/10 from test set:\n",
      "Sample: E \tPrediction: E\n",
      "Label probabilities (%): [[  0.   0. 100.   0.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 8/10 from test set:\n",
      "Sample: A \tPrediction: A\n",
      "Label probabilities (%): [[  0. 100.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 9/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[  0.   0.   0.   0.   0.   0. 100.]]\n",
      "\n",
      "Single-sample inference 10/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[  0.   0.   0.   0.   0.   0. 100.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_hist = np.array(training_results)\n",
    "validation_hist = np.array(validation_results)\n",
    "\n",
    "# best training and validation at best training\n",
    "acc_best_train = np.max(training_hist[:,1])\n",
    "epoch_best_train = np.argmax(training_hist[:,1])\n",
    "acc_val_at_best_train = validation_hist[epoch_best_train][1]\n",
    "\n",
    "# best validation and training at best validation\n",
    "acc_best_val = np.max(validation_hist[:,1])\n",
    "epoch_best_val = np.argmax(validation_hist[:,1])\n",
    "acc_train_at_best_val = training_hist[epoch_best_val][1]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Overall results:\")\n",
    "print(\"\\tBest training accuracy: {}% ({}% corresponding validation accuracy) at epoch {}/{}\".format(\n",
    "    np.round(acc_best_train*100,4), np.round(acc_val_at_best_train*100,4), epoch_best_train+1, num_epochs))\n",
    "print(\"\\tBest validation accuracy: {}% ({}% corresponding training accuracy) at epoch {}/{}\".format(\n",
    "    np.round(acc_best_val*100,4), np.round(acc_train_at_best_val*100,4), epoch_best_val+1, num_epochs))\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Test\n",
    "test_results = val_test_loop(ds_test, batch_size, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, regularization=regularization)\n",
    "print(\"Test accuracy: {}%\\n\".format(np.round(test_results[1]*100,2)))\n",
    "\n",
    "# Ns single-sample inferences to check label probabilities\n",
    "Ns = 10\n",
    "for ii in range(Ns):\n",
    "    single_sample = next(iter(DataLoader(ds_test, batch_size=1, shuffle=True)))\n",
    "    _, lbl_probs = val_test_loop(TensorDataset(single_sample[0],single_sample[1]), 1, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, label_probabilities=True, regularization=regularization)\n",
    "    print(\"Single-sample inference {}/{} from test set:\".format(ii+1,Ns))\n",
    "    print(\"Sample: {} \\tPrediction: {}\".format(letter_written[single_sample[1]],letter_written[torch.max(lbl_probs.cpu(),1)[1]]))\n",
    "    print(\"Label probabilities (%): {}\\n\".format(np.round(np.array(lbl_probs.detach().cpu().numpy())*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the trained weights\n",
    "if store_weights:\n",
    "    torch.save(best_val_layers, \"data/retrained_snntorch_{}.pt\".format(training_datetime))\n",
    "    print(\"*** weights stored ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** weights stored ***\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_val_layers, \"data/retrained_snntorch_{}.pt\".format(training_datetime))\n",
    "print(\"*** weights stored ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_NIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
