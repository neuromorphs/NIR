{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import surrogate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVICE SETTINGS\n",
    "use_gpu = False\n",
    "\n",
    "if use_gpu:\n",
    "    gpu_sel = 0\n",
    "    device = torch.device(\"cuda:\"+str(gpu_sel))\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIFY THE RESET MECHANISM TO USE\n",
    "reset_mechanism = \"subtract\" # \"zero\" or \"subtract\"\n",
    "reset_delay = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMAL HYPERPARAMETERS\n",
    "parameters_path = \"data/parameters_noDelay_noBias_ref_{}.json\".format(reset_mechanism)\n",
    "\n",
    "with open(parameters_path) as f:\n",
    "   parameters = json.load(f)\n",
    "\n",
    "parameters[\"reset\"] = reset_mechanism\n",
    "parameters[\"reset_delay\"] = reset_delay\n",
    "\n",
    "regularization = [parameters[\"reg_l1\"], parameters[\"reg_l2\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINED WEIGHTS\n",
    "#saved_state_dict_path = \"data/model_ref_{}.pt\".format(reset_mechanism)\n",
    "saved_state_dict_path = \"data/model_noDelay_noBias_ref_{}.pt\".format(reset_mechanism)\n",
    "best_val_layers = torch.load(saved_state_dict_path, map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOSS FUNCTION\n",
    "loss_fn = SF.ce_count_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST DATA\n",
    "test_data_path = \"data/ds_test.pt\"\n",
    "ds_test = torch.load(test_data_path)\n",
    "\n",
    "letter_written = ['Space', 'A', 'E', 'I', 'O', 'U', 'Y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(settings, input_size, num_steps, device):\n",
    "\n",
    "    ### Network structure (input data --> encoding -> hidden -> output)\n",
    "    input_channels = int(input_size)\n",
    "    num_hidden = int(settings[\"nb_hidden\"])\n",
    "    num_outputs = 7\n",
    "\n",
    "    ### Surrogate gradient setting\n",
    "    spike_grad = surrogate.fast_sigmoid(slope=int(settings[\"slope\"]))\n",
    "\n",
    "    ### Put things together\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            ##### Initialize layers #####\n",
    "            self.fc1 = nn.Linear(input_channels, num_hidden)\n",
    "            self.fc1.__setattr__(\"bias\",None)\n",
    "            #self.lif1 = snn.RLeaky(beta=settings[\"beta_r\"], linear_features=num_hidden, spike_grad=spike_grad, reset_mechanism=settings[\"reset\"])\n",
    "            self.lif1 = snn.RSynaptic(alpha=settings[\"alpha_r\"], beta=settings[\"beta_r\"], linear_features=num_hidden, spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "            self.lif1.recurrent.__setattr__(\"bias\",None)\n",
    "            ### Output layer\n",
    "            self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "            self.fc2.__setattr__(\"bias\",None)\n",
    "            #self.lif2 = snn.Leaky(beta=settings[\"beta_out\"], reset_mechanism=settings[\"reset\"])\n",
    "            self.lif2 = snn.Synaptic(alpha=settings[\"alpha_out\"], beta=settings[\"beta_out\"], spike_grad=spike_grad, reset_mechanism=settings[\"reset\"], reset_delay=settings[\"reset_delay\"])\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            ##### Initialize hidden states at t=0 #####\n",
    "            #spk1, mem1 = self.lif1.init_rleaky()\n",
    "            spk1, syn1, mem1 = self.lif1.init_rsynaptic()\n",
    "            #mem2 = self.lif2.init_leaky()\n",
    "            syn2, mem2 = self.lif2.init_synaptic()\n",
    "\n",
    "            # Record the spikes from the hidden layer (if needed)\n",
    "            spk1_rec = [] # not necessarily needed for inference\n",
    "            # Record the final layer\n",
    "            spk2_rec = []\n",
    "            #syn2_rec = [] # not necessarily needed for inference\n",
    "            #mem2_rec = [] # not necessarily needed for inference\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                ### Recurrent layer\n",
    "                cur1 = self.fc1(x[step])\n",
    "                #spk1, mem1 = self.lif1(cur1, spk1, mem1)\n",
    "                spk1, syn1, mem1 = self.lif1(cur1, spk1, syn1, mem1)\n",
    "                ### Output layer\n",
    "                cur2 = self.fc2(spk1)\n",
    "                #spk2, mem2 = self.lif2(cur2, mem2)\n",
    "                spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "\n",
    "                spk1_rec.append(spk1) # not necessarily needed for inference\n",
    "                spk2_rec.append(spk2)\n",
    "                #syn2_rec.append(mem2) # not necessarily needed for inference\n",
    "                #mem2_rec.append(mem2) # not necessarily needed for inference\n",
    "\n",
    "            return torch.stack(spk2_rec, dim=0), torch.stack(spk1_rec, dim=0)\n",
    "\n",
    "    return Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_test_loop(dataset, batch_size, net, loss_fn, device, shuffle=True, saved_state_dict=None, label_probabilities=False, regularization=None):\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    if saved_state_dict != None:\n",
    "        net.load_state_dict(saved_state_dict)\n",
    "    net.eval()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "\n",
    "    for data, labels in loader:\n",
    "        data = data.to(device).swapaxes(1, 0)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        spk_out, hid_rec = net(data)\n",
    "\n",
    "        return spk_out, hid_rec\n",
    "\n",
    "        # Validation loss\n",
    "        if regularization != None:\n",
    "            # L1 loss on spikes per neuron from the hidden layer\n",
    "            reg_loss = regularization[0]*torch.mean(torch.sum(hid_rec, 0))\n",
    "            # L2 loss on total number of spikes from the hidden layer\n",
    "            reg_loss = reg_loss + regularization[1]*torch.mean(torch.sum(torch.sum(hid_rec, dim=0), dim=1)**2)\n",
    "            loss_val = loss_fn(spk_out, labels) + reg_loss\n",
    "        else:\n",
    "            loss_val = loss_fn(spk_out, labels)\n",
    "\n",
    "        batch_loss.append(loss_val.detach().cpu().item())\n",
    "\n",
    "        # Accuracy\n",
    "        act_total_out = torch.sum(spk_out, 0)  # sum over time\n",
    "        _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
    "        batch_acc.extend((neuron_max_act_total_out == labels).detach().cpu().numpy()) # batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
    "    \n",
    "    if label_probabilities:\n",
    "        log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "        log_p_y = log_softmax_fn(act_total_out)\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)], torch.exp(log_p_y)\n",
    "    else:\n",
    "        return [np.mean(batch_loss), np.mean(batch_acc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INFERENCE ON TEST SET\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "input_size = 12 \n",
    "num_steps = next(iter(ds_test))[0].shape[0]\n",
    "\n",
    "net = model_build(parameters, input_size, num_steps, device)\n",
    "\n",
    "spk2, spk1 = val_test_loop(ds_test, batch_size, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, regularization=regularization)\n",
    "# print(\"Test accuracy: {}%\".format(np.round(test_results[1]*100,2)))\n",
    "# np.save('snntorch_accuracy_noDelay_noBias_subtract.npy', test_results[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 1, 40]), tensor(118.), torch.Size([256, 1, 7]), tensor(132.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spk1.shape, spk1.sum(), spk2.shape, spk2.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 40)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('snntorch_activity_noDelay_noBias_subtract.npy', spk1.squeeze(1).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-sample inference 1/10 from test set:\n",
      "Sample: Space \tPrediction: Space\n",
      "Label probabilities (%): [[100.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 2/10 from test set:\n",
      "Sample: I \tPrediction: I\n",
      "Label probabilities (%): [[  0.   0.   0. 100.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 3/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[  0.   0.   0.   0.   0.   0. 100.]]\n",
      "\n",
      "Single-sample inference 4/10 from test set:\n",
      "Sample: O \tPrediction: O\n",
      "Label probabilities (%): [[0.000e+00 0.000e+00 9.000e-02 0.000e+00 9.991e+01 0.000e+00 0.000e+00]]\n",
      "\n",
      "Single-sample inference 5/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[  0.   0.   0.   0.   0.   0. 100.]]\n",
      "\n",
      "Single-sample inference 6/10 from test set:\n",
      "Sample: Y \tPrediction: Y\n",
      "Label probabilities (%): [[  0.   0.   0.   0.   0.   0. 100.]]\n",
      "\n",
      "Single-sample inference 7/10 from test set:\n",
      "Sample: Space \tPrediction: Space\n",
      "Label probabilities (%): [[9.991e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 9.000e-02 0.000e+00]]\n",
      "\n",
      "Single-sample inference 8/10 from test set:\n",
      "Sample: I \tPrediction: I\n",
      "Label probabilities (%): [[  0.   0.   0. 100.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 9/10 from test set:\n",
      "Sample: A \tPrediction: A\n",
      "Label probabilities (%): [[  0. 100.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "Single-sample inference 10/10 from test set:\n",
      "Sample: Space \tPrediction: Space\n",
      "Label probabilities (%): [[100.   0.   0.   0.   0.   0.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### INFERENCE ON INDIVIDUAL TEST SAMPLES\n",
    "\n",
    "Ns = 10\n",
    "\n",
    "for ii in range(Ns):\n",
    "\n",
    "    single_sample = next(iter(DataLoader(ds_test, batch_size=1, shuffle=True)))\n",
    "    _, lbl_probs = val_test_loop(TensorDataset(single_sample[0],single_sample[1]), 1, net, loss_fn, device, shuffle=False, saved_state_dict=best_val_layers, label_probabilities=True, regularization=regularization)\n",
    "    print(\"Single-sample inference {}/{} from test set:\".format(ii+1,Ns))\n",
    "    print(\"Sample: {} \\tPrediction: {}\".format(letter_written[single_sample[1]],letter_written[torch.max(lbl_probs.cpu(),1)[1]]))\n",
    "    print(\"Label probabilities (%): {}\\n\".format(np.round(np.array(lbl_probs.detach().cpu().numpy())*100,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this requires snntorch/nir (PR) and nirtorch/master (unreleased)\n",
    "from snntorch import export_nir\n",
    "\n",
    "\n",
    "nir_graph = export_nir.to_nir(net, ds_test[0][0])\n",
    "\n",
    "print('nodes:')\n",
    "for nodekey, node in nir_graph.nodes.items():\n",
    "    print('\\t', nodekey, node.__class__.__name__)\n",
    "print('edges:')\n",
    "for edge in nir_graph.edges:\n",
    "    print('\\t', edge)\n",
    "\n",
    "import nir\n",
    "nir.write('braille.nir', nir_graph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
