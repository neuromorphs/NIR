{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(dev_pytorch)=\n",
    "# PyTorch via NIRTorch\n",
    "\n",
    "PyTorch is a popular deep learning framework that many of the NIR-supported libraries are built on.\n",
    "We have built the [`nirtorch` package](https://github.com/neuromorphs/nirtorch) to make it easier to develop PyTorch extensions for the NIR-supported libraries.\n",
    "`nirtorch` helps you write PyTorch code that (1) exports NIR models from PyTorch and (2) imports NIR models into PyTorch.\n",
    "\n",
    "## Exporting NIR models from PyTorch\n",
    "\n",
    "```{admonition} See also\n",
    ":class: seealso\n",
    "Read more about exporting NIR models from PyTorch in the page about [NIR Tracing with NIRTorch](#nirtorch_tracing).\n",
    "```\n",
    "\n",
    "Exporting a NIR model requires two things: exporting the model's nodes and edges.\n",
    "\n",
    "### Exporting edges\n",
    "Exporting edges is slightly complicated because PyTorch modules can have multiple inputs and outputs.\n",
    "And because PyTorch modules are connected via function calls, which only happen at runtime.\n",
    "Therefore, we need to trace the PyTorch module to get the edges with some sample input.\n",
    "Luckily, `nirtorch` package helps you do exactly that.\n",
    "It works behind the scenes, but you can read more about it in [To NIR: Tracing PyTorch](#nirtorch_tracing).\n",
    "\n",
    "### Exporting nodes\n",
    "Exporting nodes in PyTorch is typically a 1:1 mapping between the PyTorch module and the NIR node.\n",
    "This is done in `nirtorch` by simply providing a function for each PyTorch module that returns the corresponding NIR node.\n",
    "In Python types, this is a `Dictionary[torch.nn.Module, Callable[[torch.nn.Module], nir.NIRNode]]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nir\n",
    "import torch\n",
    "\n",
    "class MyLeakyIntegrator(torch.nn.Module):\n",
    "    tau: torch.Tensor\n",
    "    r: torch.Tensor\n",
    "    v_leak: torch.Tensor\n",
    "\n",
    "    def __init__(self, tau, r, v_leak):\n",
    "        super().__init__() # Required for subclasses of torch.nn.Module\n",
    "        self.tau=tau\n",
    "        self.r=r\n",
    "        self.v_leak=v_leak\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        if state is None:\n",
    "            state = torch.tensor([0.])\n",
    "        x = self.tau * (self.v_leak - state + self.r * x)\n",
    "        return x, x # Return both output and state\n",
    "\n",
    "my_torch_dictionary = {\n",
    "    MyLeakyIntegrator: lambda module: nir.LI(tau=tau, r=r, v_leak=v_leak)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Why does the forward method have a <code>state</code> parameter?\n",
    ":class: seealso\n",
    "Read more about the structure of `MyLeakyIntegrator` in the [\"Stateful execution\"](#stateful-execution) section below or in the page on [State management in NIRTorch](#nirtorch_state)\n",
    "```\n",
    "\n",
    "The dictionary `my_torch_dictionary` basically explains how to convert a custom `MyLeakyIntegrator` module to a NIR LI (leaky integrator) node.\n",
    "Note that we only have to add entries for nodes that we support and want to export.\n",
    "If we do not support modules, we can leave them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "We can now do a [short, self-contained example](https://www.sscce.org/) for exporting a NIR node using `nirtorch`.\n",
    "Recall that the edges are traced automatically by the `nirtorch` package, so the only thing we really have to define is the dictionary defined above, `my_torch_dictionary`.\n",
    "The rest is taken care of by `nirtorch`'s `torch_to_nir` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LI(tau=tensor(1.), r=tensor(1.), v_leak=tensor(1.), input_type={'input': array([], dtype=float64)}, output_type={'output': array([], dtype=float64)}, metadata={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nir, nirtorch, norse, torch\n",
    "\n",
    "class MyLeakyIntegrator(torch.nn.Module):\n",
    "    tau: torch.Tensor\n",
    "    r: torch.Tensor\n",
    "    v_leak: torch.Tensor\n",
    "\n",
    "    def __init__(self, tau, r, v_leak):\n",
    "        super().__init__() # Required for subclasses of torch.nn.Module\n",
    "        self.tau=tau\n",
    "        self.r=r\n",
    "        self.v_leak=v_leak\n",
    "\n",
    "my_torch_dictionary = {\n",
    "    MyLeakyIntegrator: lambda module: nir.LI(tau=module.tau, r=module.r, v_leak=module.v_leak)\n",
    "}\n",
    "\n",
    "# Create some mock data\n",
    "tau, r, v_leak = torch.ones(3)\n",
    "# ... And an example module\n",
    "my_module = MyLeakyIntegrator(tau, r, v_leak)\n",
    "\n",
    "# Use nirtorch to map my_module using my_torch_dictionary to convert modules\n",
    "my_nir_graph = nirtorch.torch_to_nir(my_module, my_torch_dictionary)\n",
    "my_nir_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a NIR graph.\n",
    "You can inspect it by exploring the nodes and edges (see how to work with nodes and edges in [Working with NIR](#working_with_nir)) or send it to another platform for continued proccesing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NIR models into PyTorch\n",
    "```{admonition} See also\n",
    ":class: seealso\n",
    "Read more about importing NIR models into PyTorch on the page about [To PyTorch: Interpreting NIR](#nirtorch_interpreting).\n",
    "```\n",
    "\n",
    "Assuming you have a NIR graph in the Python object `nir_graph` (see @using_nir), we need to inform `nirtorch` how to map NIR nodes into your simulator.\n",
    "That is, for each node, we need a function (`nir.NIRNode -> torch.nn.Module`) which is given by a dictionary of type `Dictionary[nir.NIRNode, Callable[[nir.NIRNode], torch.nn.Module]]` (read about why in [To PyTorch: Interpreting NIR](#nirtorch_interpreting)).\n",
    "With that dictionary, we can call `nirtorch`'s `nir_to_torch` method with the NIR node we want to map.\n",
    "Here's a complete example where we are defining a simple mapper for the `nir.LI` module that a `MyLeakyIntegrator` module (also used above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8656, 1.9804, 1.8317, 1.0708, 1.9365, 1.4059, 1.6528, 1.6479, 1.3619,\n",
       "        1.0023], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nir, nirtorch, numpy as np, torch\n",
    "\n",
    "class MyLeakyIntegrator(torch.nn.Module):\n",
    "    tau: torch.Tensor\n",
    "    r: torch.Tensor\n",
    "    v_leak: torch.Tensor\n",
    "\n",
    "    def __init__(self, tau, r, v_leak):\n",
    "        super().__init__() # Required for subclasses of torch.nn.Module\n",
    "        self.tau=tau\n",
    "        self.r=r\n",
    "        self.v_leak=v_leak\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        if state is None:\n",
    "            state = torch.tensor([0.])\n",
    "        x = self.tau * (self.v_leak - state + self.r * x)\n",
    "        return x, x # Return both output and state\n",
    "        \n",
    "\n",
    "my_nir_dictionary = {\n",
    "    nir.LI: lambda node: MyLeakyIntegrator(torch.from_numpy(node.tau), torch.from_numpy(node.r), torch.from_numpy(node.v_leak))\n",
    "}\n",
    "\n",
    "tau = np.ones(1)\n",
    "r = np.ones(1)\n",
    "v_leak = np.ones(1)\n",
    "my_nir_graph = nir.NIRGraph.from_list(nir.LI(tau, r, v_leak))\n",
    "\n",
    "my_torch_module = nirtorch.nir_to_torch(my_nir_graph, my_nir_dictionary)\n",
    "\n",
    "# I can now execute the torch module\n",
    "output, state = my_torch_module(torch.rand(10))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(stateful-execution)=\n",
    "### Stateful execution\n",
    "```{admonition} See also\n",
    ":class: seealso\n",
    "Read more about state handling with NIRTorch on the page about [State handling with NIRTorch](#nirtorch_state).\n",
    "```\n",
    "Note the stateful execution above, both in the `MyLeakyIntegrator` and in the second parameter in the call to `my_torch_module`!\n",
    "Many NIR primitives can be seen as recurrent neurons, which require us to maintain state.\n",
    "That can be done either implicitly or explicitly.\n",
    "**Implicit** state handling means setting some variable in the module that automatically gets updated so the user does not have to worry about it.\n",
    "The downside is that the user does not have any control over it and *may forget to reset the state*. The worst-case is that the module behaves wrongly without the user noticing.\n",
    "**Explicit** state handling requires that state is both sent as input and returned as output. Typically, this means that the module requires two inputs (data + state) and returns a tuple of `(data, state)`.\n",
    "This grants complete control to the user with the downside that the user has to handle the state.\n",
    "\n",
    "Since some PyTorch libraries explicitly declare state, `nirtorch` uses the **explicit** state handling method.\n",
    "Specifically, the state is a dictionary where each entry correspond to the state of each submodule.\n",
    "The state may contain multiple levels, if the module has submodules etc. Read more about the distinction between implicit and explicit state, as well as how this is handled in `nirtorch` in the page on [State management in NIRTorch](#nirtorch_state)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
